{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from utils import get_llm_response, load_json, write_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Belief and prediction generation prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def get_dict(text):\n",
    "    return re.findall(r'{[\\s\\S]*}', text)[0]\n",
    "\n",
    "belief_prompt = (\n",
    "    \"Give a short answer to the following question. Your answer should be in the format {{\\\"Explanation\\\": \\\"ANSWER EXPLANATION\\\", \\\"Answer\\\": \\\"ANSWER TEXT\\\"}}:\\n\"\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A:\"\n",
    ")\n",
    "def get_model_belief(question, model=\"llama3-70b-instruct\"):\n",
    "    res = get_llm_response(belief_prompt.format(question=question), model=model)\n",
    "    return json.loads(get_dict(res))\n",
    "\n",
    "predict_prompt = (\n",
    "    \"Given two related evidence, provide a short answer to the following question. Your answer should be in the format {{\\\"Explanation\\\": \\\"ANSWER EXPLANATION\\\", \\\"Answer\\\": \\\"ANSWER TEXT\\\"}}:\\n\"\n",
    "    \"Evidence 1: {e1}\\n\"\n",
    "    \"Evidence 2: {e2}\\n\"\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A:\"\n",
    ")\n",
    "def get_model_prediction(question, e1, e2, model=\"llama3-70b-instruct\"):\n",
    "    res = get_llm_response(predict_prompt.format(question=question, e1=e1, e2=e2), model=model)\n",
    "    return json.loads(get_dict(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../annotations/answer_conflict_annotations.csv\")\n",
    "df = df[df['data_label']==1]\n",
    "df = df[df['annotator_majority']==1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "for model in [\"llama3-8b-instruct\", \"llama3-70b-instruct\", \"claude-v3-haiku\", \"claude-v3-sonnet\"]:\n",
    "    prompt_records = []\n",
    "    for item in tqdm(df.to_dict(orient=\"records\")):\n",
    "        # pprint.pp(item)\n",
    "        pairs = eval(item['pairs'])\n",
    "        q = pairs[0]\n",
    "        q_end = q.index(\"?\")+1\n",
    "        q = q[: q_end]\n",
    "        e1 = pairs[0][q_end:].strip()\n",
    "        e2 = pairs[1][q_end:].strip()\n",
    "\n",
    "        answer_pairs = eval(item['answer_pairs'])\n",
    "        a1 = answer_pairs[0]\n",
    "        a2 = answer_pairs[1]\n",
    "\n",
    "        ground_truths = eval(item['ground_truths'])\n",
    "        is_gt = [a1 in ground_truths, a2 in ground_truths]\n",
    "\n",
    "        types = eval(item['annotated_type'])\n",
    "        instance_id = item['instance_id']\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                belief = get_model_belief(q, model=model)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Belief\", e)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                prediction = get_model_prediction(q, e1, e2, model)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Predict\", e)\n",
    "\n",
    "        prompt_records.append({\n",
    "            \"question\": q,\n",
    "            \"e1\": (a1, is_gt[0], e1),\n",
    "            \"e2\": (a2, is_gt[1], e2),\n",
    "            \"types\": types,\n",
    "            \"llm_belief\": belief,\n",
    "            \"llm_predict\": prediction,\n",
    "            \"model\": model,\n",
    "            \"instance_id\": instance_id\n",
    "        })\n",
    "    write_json(f\"results/{model}.json\", prompt_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Factoid-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import get_llm_response, load_json, write_json\n",
    "factoid_df = pd.read_csv(\"../annotations/factoid_conflict_annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def get_dict(text):\n",
    "    return re.findall(r'{[\\s\\S]*}', text)[0]\n",
    "\n",
    "belief_prompt = (\n",
    "    \"Give a yes/no answer to the following question. Your answer should be in the format {{\\\"Explanation\\\": \\\"ANSWER EXPLANATION\\\", \\\"Answer\\\": \\\"YES or NO or UNKNOWN\\\"}}:\\n\"\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A:\"\n",
    ")\n",
    "def get_model_belief(question, model=\"llama3-70b-instruct\"):\n",
    "    res = get_llm_response(belief_prompt.format(question=question), model=model)\n",
    "    return json.loads(get_dict(res))\n",
    "\n",
    "predict_prompt = (\n",
    "    \"Given two related evidence, provide a yes/no answer to the following question. Your answer should be in the format {{\\\"Explanation\\\": \\\"ANSWER EXPLANATION\\\", \\\"Answer\\\": \\\"YES or NO or UNKNOWN\\\"}}:\\n\"\n",
    "    \"Evidence 1: {e1}\\n\"\n",
    "    \"Evidence 2: {e2}\\n\"\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A:\"\n",
    ")\n",
    "def get_model_prediction(question, e1, e2, model=\"llama3-70b-instruct\"):\n",
    "    res = get_llm_response(predict_prompt.format(question=question, e1=e1, e2=e2), model=model)\n",
    "    return json.loads(get_dict(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "os.makedirs(\"factoid_results\", exist_ok=True)\n",
    "\n",
    "for model in [\"claude-v3-haiku\", \"claude-v3-sonnet\", \"llama3-8b-instruct\", \"llama3-70b-instruct\"]:\n",
    "    prompt_records = []\n",
    "    for item in tqdm(factoid_df.to_dict(orient=\"records\")):\n",
    "        # pprint.pp(item)\n",
    "        pairs = eval(item['pairs'])\n",
    "        q = pairs[0]\n",
    "        q_end = q.index(\"?\")+2\n",
    "        q = q[: q_end]\n",
    "        e1 = pairs[0][q_end:].strip()\n",
    "        e2 = pairs[1][q_end:].strip()\n",
    "\n",
    "        a1, a2 = eval(item['answer_pairs'])\n",
    "\n",
    "        factoid_pairs = eval(item['factoids_pairs'])\n",
    "        af1 = factoid_pairs[\"factoids_1\"]\n",
    "        af2 = factoid_pairs[\"factoids_2\"]\n",
    "\n",
    "        ground_truths = item['ground_truths']\n",
    "        # is_gt = [1, 0]\n",
    "\n",
    "        types = eval(item['annotated_type'])\n",
    "        instance_id = item['instance_id']\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                belief = get_model_belief(q, model=model)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Belief\", e)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                prediction = get_model_prediction(q, e1, e2, model)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Predict\", e)\n",
    "\n",
    "        prompt_records.append({\n",
    "            \"question\": q,\n",
    "            \"e1\": (a1, af1,  e1),\n",
    "            \"e2\": (a2, af2,  e2),\n",
    "            \"a\": ground_truths,\n",
    "            \"types\": types,\n",
    "            \"llm_belief\": belief,\n",
    "            \"llm_predict\": prediction,\n",
    "            \"model\": model,\n",
    "            \"instance_id\": instance_id\n",
    "        })\n",
    "    write_json(f\"factoid_results/{model}.json\", prompt_records)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
